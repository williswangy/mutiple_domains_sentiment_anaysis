{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LzdLChpJGqX"
      },
      "source": [
        "## GOOGLE DRIVE IMPORT *DATASETS*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpdRGUU4EKuo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change the current working directory to 'transfer_learning'\n",
        "os.chdir('/content/drive/My Drive/transfer_learning')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wVShyyZu6Gb"
      },
      "source": [
        "## Distribution of sentiment labels in training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MovYGRLoFmR3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Load the DataFrames\n",
        "df1 = pd.read_csv('data/covid_data_labeled.csv')\n",
        "df2 = pd.read_csv('data/mental_health_modified_data.csv')\n",
        "df3 = pd.read_csv('data/drug_cleaned_data.csv')\n",
        "\n",
        "# Function to prepare the data for visualization\n",
        "def prepare_data(df):\n",
        "    # Replace labels for consistency\n",
        "    if 'label' in df.columns:\n",
        "        df['label'] = df['label'].replace({0: 'negative', 2: 'positive', 1: 'neutral'})\n",
        "    counts = df['label'].value_counts().reset_index()\n",
        "    counts.columns = ['Labels', 'Count']\n",
        "    return counts\n",
        "\n",
        "# Prepare data\n",
        "df1_counts = prepare_data(df1)\n",
        "df2_counts = prepare_data(df2)\n",
        "df3_counts = prepare_data(df3)\n",
        "\n",
        "# Set up the figure and axes\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Custom function to draw bar plots on the given axis\n",
        "def plot_on_axis(ax, data, title):\n",
        "    sns.barplot(x='Count', y='Labels', data=data, palette='pastel', ax=ax)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Number of Samples (in thousands)')\n",
        "    ax.set_ylabel('')\n",
        "    ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\n",
        "\n",
        "# Plot the data on respective axes\n",
        "plot_on_axis(ax1, df1_counts, 'COVID Dataset')\n",
        "plot_on_axis(ax2, df2_counts, 'Mental Dataset')\n",
        "plot_on_axis(ax3, df3_counts, 'Drug Dataset')\n",
        "\n",
        "# Improve layout\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"data_visualization.pdf\", format='pdf', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9taRYNaUk7pq"
      },
      "outputs": [],
      "source": [
        "# Print out the distributions\n",
        "print(\"Distribution of COVID Data:\")\n",
        "print(df1_counts)\n",
        "print(\"\\nDistribution of Mental Health Data:\")\n",
        "print(df2_counts)\n",
        "print(\"\\nDistribution of Drug Data:\")\n",
        "print(df3_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6_pNdhXJ5AF"
      },
      "source": [
        "## ETL Pipeline\n",
        "\n",
        "In this ETL (Extract, Transform, Load) section, the dataset is processed using a suite of text cleaning methods provided by the `DataPreprocessor` class. The following transformations are performed:\n",
        "\n",
        "### Extracting\n",
        "- The data is read from a CSV file.\n",
        "\n",
        "### Transforming\n",
        "Several transformations are applied to the text data:\n",
        "\n",
        "   - Removing website links, usernames, special characters, and certain other characters from the text.\n",
        "   - Simplifying the cleaned text by removing placeholders, punctuation marks, short words, and extra whitespace.\n",
        "   - Tokenizing the text, removing stopwords, and lemmatizing the tokens.\n",
        "   - Splitting hashtags and usernames.\n",
        "   - Creating new columns for the various stages of cleaned text, reordering and renaming columns for clarity.\n",
        "\n",
        "### Loading\n",
        "- The preprocessed data, now ready for further analysis or machine learning tasks, is returned as a Pandas DataFrame.\n",
        "\n",
        "This comprehensive ETL process aims to simplify and standardize the text data, preparing it for subsequent stages in our data pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C277ERdYJ7qq"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode\n",
        "import pandas as pd\n",
        "import re\n",
        "import unidecode\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "\n",
        "class DataPreprocessor:\n",
        "    \"\"\"\n",
        "    The DataPreprocessor class provides a suite of methods to preprocess text data,\n",
        "    including cleaning the text, removing stopwords, splitting hashtags, and usernames.\n",
        "    \"\"\"\n",
        "\n",
        "    # Regular expressions to match special characters, usernames, and website links\n",
        "    SPECIAL_CHARS_REGEX = r\"[\\*\\+'\\/\\(\\)\\]\\[\\_\\|]\"\n",
        "    USERNAME_REGEX = r'@\\w*'\n",
        "    WEBSITE_REGEX = r'http\\S*'\n",
        "\n",
        "    # Create an instance of TweetTokenizer\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
        "    @staticmethod\n",
        "    def remove_usernames_weblinks_and_special_chars(text):\n",
        "        \"\"\"\n",
        "        Replaces usernames and website links in the given text with placeholders,\n",
        "        replaces \"&amp\" with empty string, special characters with spaces, and removes apostrophes.\n",
        "\n",
        "        Args:\n",
        "            text (str): The original text string.\n",
        "\n",
        "        Returns:\n",
        "            str: The cleaned text string.\n",
        "        \"\"\"\n",
        "        # Replace usernames and website links with placeholders\n",
        "        text = re.sub(DataPreprocessor.USERNAME_REGEX, 'USERNAME', text)\n",
        "        text = re.sub(DataPreprocessor.WEBSITE_REGEX, 'WEBSITE', text)\n",
        "\n",
        "        # Replace \"&amp\" with empty string\n",
        "        text = text.replace(\"&amp\", '')\n",
        "\n",
        "        # Replace special characters with spaces\n",
        "        text = re.sub(DataPreprocessor.SPECIAL_CHARS_REGEX, ' ', text)\n",
        "\n",
        "        # Remove apostrophes\n",
        "        text = text.replace(\"'\", \"\")\n",
        "\n",
        "        # Replace hyphens, commas, and ampersands with spaces\n",
        "        text = re.sub(r\"[-&,]\", ' ', text)\n",
        "\n",
        "        # Replace punctuation marks with periods\n",
        "        text = re.sub(r\"[:;?!]\", '.', text)\n",
        "\n",
        "        # Replace multiple periods with a single period\n",
        "        text = re.sub(r'\\.+', '.', text)\n",
        "\n",
        "        # Replace multiple periods separated by spaces with a single period\n",
        "        text = re.sub(r'\\. \\.+', '.', text)\n",
        "\n",
        "        return text.strip()\n",
        "    @staticmethod\n",
        "    def simplify_text(text):\n",
        "        \"\"\"\n",
        "        Simplifies the cleaned text by removing placeholders for usernames and website links,\n",
        "        removing punctuation marks, and eliminating short words and extra whitespace.\n",
        "\n",
        "        Args:\n",
        "            text (str): The cleaned text string.\n",
        "\n",
        "        Returns:\n",
        "            str: The further simplified text string.\n",
        "        \"\"\"\n",
        "        # Replace usernames and website links with empty strings\n",
        "        text = re.sub(\"USERNAME\", '', text)\n",
        "        text = re.sub(\"WEBSITE\", '', text)\n",
        "\n",
        "        # Remove punctuation marks, short words, and extra whitespace\n",
        "        text = re.sub(r\"\\b\\w{1,2}\\b\", '', text)\n",
        "        text = re.sub(r\"\\s\\s+\", ' ', text)\n",
        "        text = text.translate(str.maketrans(\n",
        "            '', '', '!\"$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'))\n",
        "        return text.strip()\n",
        "    @staticmethod\n",
        "    def eliminate_stopwords(tokens):\n",
        "        \"\"\"\n",
        "        Removes the stopwords from the given list of tokens and lemmatizes the words for normalization.\n",
        "\n",
        "        Args:\n",
        "            tokens (list): A list of word tokens.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of tokens with stopwords removed.\n",
        "        \"\"\"\n",
        "        stop = stopwords.words('english')\n",
        "        stop.extend(['from', 'subject', 're', 'edu', 'use', 'via', 'like', 'ha'])\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        new_tokens = []\n",
        "\n",
        "        for token in tokens:\n",
        "            # Handle contractions\n",
        "            if \"'\" in token:\n",
        "                parts = token.split(\"'\")\n",
        "                if parts[1].lower() == \"t\":\n",
        "                    token = parts[0] + \" not\"\n",
        "                elif parts[1].lower() == \"ve\":\n",
        "                    token = parts[0] + \" have\"\n",
        "\n",
        "            # Lemmatize token\n",
        "            lemma = lemmatizer.lemmatize(token)\n",
        "\n",
        "            # Check if token is a stopword or lemma\n",
        "            if lemma not in stop:\n",
        "                new_tokens.append(lemma)\n",
        "\n",
        "        return new_tokens\n",
        "    @staticmethod\n",
        "    def separate_hashtags_usernames(text):\n",
        "        \"\"\"\n",
        "        Splits hashtags and usernames in the given text.\n",
        "\n",
        "        Args:\n",
        "            text (str): The text string containing hashtags and usernames.\n",
        "\n",
        "        Returns:\n",
        "            str: The text string with hashtags and usernames split.\n",
        "        \"\"\"\n",
        "        tokens = text.split()\n",
        "        for i in range(len(tokens)):\n",
        "            if (tokens[i][0] == '#') or (tokens[i][0] == '@'):\n",
        "                tokens[i] = tokens[i].replace('#', '')\n",
        "                tokens[i] = tokens[i].replace('@', '')\n",
        "                out = re.split(r'(?<=[a-z])(?=[A-Z])', tokens[i])\n",
        "                tokens[i] = ' '.join(out)\n",
        "        tokens = ' '.join(tokens)\n",
        "        return tokens\n",
        "    @classmethod\n",
        "    def preprocess_dataset(cls, data_file, data_type):\n",
        "        \"\"\"\n",
        "        Processes the whole dataset performing all of the cleaning and preprocessing operations on the text data.\n",
        "\n",
        "        Args:\n",
        "            data_file (str): The path to the CSV data file.\n",
        "            data_type (str): The type of data file.\n",
        "\n",
        "        Returns:\n",
        "            pandas.DataFrame: The preprocessed DataFrame.\n",
        "        \"\"\"\n",
        "        # Read in data and clean text column\n",
        "        df = pd.read_csv(data_file, quotechar='\"', encoding='utf-8')\n",
        "        # # specificlaly for sentiment 140\n",
        "        # df = pd.read_csv(data_file, encoding='latin', names=['polarity', 'id', 'date', 'query', 'user', 'text'])\n",
        "        df['clean_text'] = df['text'].astype(str).apply(cls.remove_usernames_weblinks_and_special_chars)\n",
        "\n",
        "\n",
        "        # Remove non-ASCII characters\n",
        "        df['clean_text'] = df['clean_text'].apply(unidecode.unidecode)\n",
        "        df['clean_text'] = df['clean_text'].apply(cls.separate_hashtags_usernames)\n",
        "        # Create simplified text column without usernames, websites, punctuation, and short words\n",
        "        df['clean_text_simple'] = df['clean_text'].apply(cls.simplify_text)\n",
        "\n",
        "        # Tokenize text, remove stopwords and lemmatize, and untokenize\n",
        "        df['tokens'] = df['clean_text_simple'].apply(cls.tokenizer.tokenize)\n",
        "        df['tokens'] = df['tokens'].apply(cls.eliminate_stopwords)\n",
        "        df['text_simple'] = df['tokens'].apply(' '.join)\n",
        "\n",
        "        # Remove tokens column\n",
        "        df.drop('tokens', axis=1, inplace=True)\n",
        "        # Rename columns for clarity\n",
        "        columns = {\n",
        "            'text': 'original_text',\n",
        "            'clean_text': 'clean_text_with_usernames_and_hashtags',\n",
        "            'clean_text_simple': 'clean_text_without_usernames_and_hashtags',\n",
        "            'text_simple': 'clean_text_without_usernames_hashtags_or_stopwords'\n",
        "        }\n",
        "\n",
        "        # if data_type == \"label\":\n",
        "        #     columns['polarity'] = 'label'\n",
        "        df = df.rename(columns=columns)\n",
        "\n",
        "        # Reorder columns for readability\n",
        "        cols_to_keep = ['original_text', 'clean_text_without_usernames_hashtags_or_stopwords']\n",
        "        if data_type == \"label\":\n",
        "            cols_to_keep.append('label')\n",
        "        df = df[cols_to_keep]\n",
        "\n",
        "        return df\n",
        "\n",
        "    '''\n",
        "    #Demo Usage\n",
        "    preprocessor = DataPreprocessor()\n",
        "    cleaned_data = preprocessor.preprocess_dataset(data_file='path_to_your_data.csv', data_type='your_type')\n",
        "\n",
        "    '''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azEYGD0-mH9f"
      },
      "outputs": [],
      "source": [
        "    preprocessor = DataPreprocessor()\n",
        "    cleaned_data = preprocessor.preprocess_dataset(data_file='data/drug_proportional_renamed.csv', data_type='label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrVRQAqDnFZT"
      },
      "outputs": [],
      "source": [
        "cleaned_data.to_csv('data/aug_drug_cleaned_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cunx02VvJcyc"
      },
      "source": [
        "## Data Sampling, Train-Test Split, and Preparation\n",
        "\n",
        "In this section, we perform several steps to prepare the dataset for model training and evaluation:\n",
        "\n",
        "1. **Data Import**: We begin by reading the cleaned CSV file which was generated from the previous ETL process.\n",
        "2. **Data Cleaning**: After importing the data, we remove rows that contain missing values in the 'label' column and other columns to ensure the quality of our dataset.\n",
        "3. **Train-Test Split**: We split the dataset into a training set and a testing set while maintaining the proportion of classes in each set (stratified sampling). This split is set at 90% for the training set and 10% for the testing set.\n",
        "4. **Data Transformation**: Finally, we convert the 'label' column into a categorical format by encoding it into a one-hot format, which is suitable for multi-class classification problems.\n",
        "\n",
        "By the end of these steps, our dataset is ready for model training and testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqJk1ldsJS-P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def generate_train_test(df, text_column, label_column, num_classes, test_size=0.1):\n",
        "    # Drop rows with missing values in the label column\n",
        "    df = df.dropna(subset=[label_column])\n",
        "\n",
        "    # Drop remaining rows with missing values\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df[text_column].astype(str),\n",
        "        df[label_column].astype(int),\n",
        "        test_size=test_size,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Convert labels to one-hot vectors if there's more than one class\n",
        "    if num_classes > 1:\n",
        "        y_train = to_categorical(y_train, num_classes)\n",
        "        y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Load the DataFrames\n",
        "df_covid = pd.read_csv('data/covid_cleaned_data.csv')\n",
        "# Load mental health dataset and remap the labels\n",
        "df_mental_health = pd.read_csv('data/mental_health_modified_data.csv')\n",
        "# If the labels are 0 and 2, remap 2 to 1\n",
        "df_mental_health[\"label\"] = df_mental_health[\"label\"].replace(2, 1)\n",
        "df_drug = pd.read_csv('data/drug_cleaned_data.csv')\n",
        "df_aug_drug = pd.read_csv('data/aug_drug_cleaned_data.csv')\n",
        "\n",
        "# Generate training and testing data\n",
        "X_train_covid, X_test_covid, y_train_covid, y_test_covid = generate_train_test(df_covid, \"clean_text_without_usernames_hashtags_or_stopwords\", \"label\", num_classes=3)\n",
        "print(f\"Training Size (COVID Data): {len(X_train_covid)}\")\n",
        "\n",
        "X_train_mental, X_test_mental, y_train_mental, y_test_mental = generate_train_test(df_mental_health, \"clean_text_without_usernames_hashtags_or_stopwords\", \"label\", num_classes=2)\n",
        "print(f\"Training Size (Mental Health Data): {len(X_train_mental)}\")\n",
        "\n",
        "X_train_drug, X_test_drug, y_train_drug, y_test_drug = generate_train_test(df_drug, \"clean_text_without_usernames_hashtags_or_stopwords\", \"label\", num_classes=3)\n",
        "print(f\"Training Size (Drug Data): {len(X_train_drug)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8DWeSaX9qGx"
      },
      "outputs": [],
      "source": [
        "df_drug['label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OY1xxFCPyPB"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl_cgW7sPyWB"
      },
      "source": [
        "### Data Augmentation for drug dataset\n",
        "\n",
        "In this segment, we execute a series of actions to enhance our textual data for more robust model performance:\n",
        "\n",
        "- **Synonym Extraction**: Leveraging the WordNet database, the `get_synonyms()` function identifies synonyms for a given word, enhancing data diversity.\n",
        "\n",
        "#### Text Augmentation Techniques:\n",
        "\n",
        "  - `synonym_replacement()`: Alters random words in a text with their corresponding synonyms.\n",
        "  - `random_insertion()`: Embeds random synonyms at unspecified positions within a text.\n",
        "  - `random_swap()`: Interchanges random word pairs in a text.\n",
        "  - `random_deletion()`: Randomly omits words from a text based on a predetermined probability.\n",
        "\n",
        "- **Augmentation Application**: The `augment_text()` function selects a random augmentation technique and implements it, introducing controlled variability in our dataset.\n",
        "\n",
        "- **Augmentation Volume Calculation**: The `calculate_augmented_counts()` function ascertains the required volume of augmented samples for each class, ensuring retention of the original class distribution.\n",
        "\n",
        "- **Proportional Augmentation**: Using `proportional_augmentation()`, we achieve a dataset enriched with augmented text, preserving the original class distribution. In this context, the drug dataset (`df_drug`) is targeted for a quadrupled increase in size.\n",
        "\n",
        "By the conclusion of these procedures, our dataset is enriched and diversified, standing ready for advanced modeling tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-4_5K0v6jzo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download WordNet data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
        "            synonyms.add(synonym)\n",
        "    if word in synonyms:\n",
        "        synonyms.remove(word)\n",
        "    return list(synonyms)\n",
        "\n",
        "def synonym_replacement(words, n=5):\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set([word for word in words]))\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replaced = 0\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = get_synonyms(random_word)\n",
        "        if len(synonyms) >= 1:\n",
        "            synonym = random.choice(list(synonyms))\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "    sentence = ' '.join(new_words)\n",
        "    return sentence\n",
        "\n",
        "def random_insertion(words, n=3):\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        add_word(new_words)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def add_word(new_words):\n",
        "    synonyms = []\n",
        "    counter = 0\n",
        "    while len(synonyms) < 1 and counter < 10:\n",
        "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
        "        synonyms = get_synonyms(random_word)\n",
        "        counter += 1\n",
        "    if len(synonyms) >= 1:\n",
        "        random_synonym = random.choice(list(synonyms))\n",
        "        random_idx = random.randint(0, len(new_words)-1)\n",
        "        new_words.insert(random_idx, random_synonym)\n",
        "\n",
        "def random_swap(words, n=3):\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        new_words = swap_word(new_words)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def swap_word(new_words):\n",
        "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
        "    random_idx_2 = random_idx_1\n",
        "    counter = 0\n",
        "    while random_idx_2 == random_idx_1 and counter < 3:\n",
        "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
        "        counter += 1\n",
        "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
        "    return new_words\n",
        "\n",
        "def random_deletion(words, p=0.5):\n",
        "    if len(words) == 1:\n",
        "        return ' '.join(words)\n",
        "    remaining = list(filter(lambda x: random.uniform(0, 1) > p, words))\n",
        "    if len(remaining) == 0:\n",
        "        return random.choice(words)\n",
        "    else:\n",
        "        return ' '.join(remaining)\n",
        "\n",
        "def augment_text(text):\n",
        "    methods = [synonym_replacement, random_insertion, random_swap, random_deletion]\n",
        "    random_method = random.choice(methods)\n",
        "    words = text.split(' ')\n",
        "    return random_method(words)\n",
        "\n",
        "def calculate_augmented_counts(original_counts, total_target):\n",
        "    \"\"\"\n",
        "    Calculate the number of samples needed for each class to maintain the original proportion.\n",
        "    \"\"\"\n",
        "    total_original = sum(original_counts.values)\n",
        "    proportions = original_counts / total_original\n",
        "    target_counts = (total_target * proportions).round().astype(int)\n",
        "    return target_counts\n",
        "\n",
        "def proportional_augmentation(df, original_text_col, label_col, total_target_size):\n",
        "    augmented_texts = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    original_counts = df[label_col].value_counts()\n",
        "    target_counts = calculate_augmented_counts(original_counts, total_target_size)\n",
        "\n",
        "    for label, target in target_counts.iteritems():\n",
        "        samples_to_add = target - original_counts[label]\n",
        "\n",
        "        if samples_to_add <= 0:\n",
        "            continue\n",
        "\n",
        "        class_texts = df[df[label_col] == label][original_text_col].values\n",
        "\n",
        "        for _ in range(samples_to_add):\n",
        "            sample_text = np.random.choice(class_texts)\n",
        "            augmented_texts.append(augment_text(sample_text))\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "    df_augmented = pd.DataFrame({\n",
        "        original_text_col: augmented_texts + df[original_text_col].tolist(),\n",
        "        label_col: augmented_labels + df[label_col].tolist()\n",
        "    })\n",
        "\n",
        "    return df_augmented\n",
        "\n",
        "# Assuming df_drug is already loaded\n",
        "total_target_size = len(df_drug) * 4\n",
        "\n",
        "df_drug_proportional_augmented = proportional_augmentation(df_drug, \"original_text\", \"label\", total_target_size)\n",
        "\n",
        "# Checking the shape of the augmented dataframe\n",
        "print(f\"Shape of the proportionally augmented dataframe: {df_drug_proportional_augmented.shape}\")\n",
        "\n",
        "# Checking the distribution of the classes\n",
        "class_distribution_proportional = df_drug_proportional_augmented['label'].value_counts()\n",
        "print(\"\\nClass distribution in the proportionally augmented dataframe:\")\n",
        "print(class_distribution_proportional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHj4lvakRgUa"
      },
      "outputs": [],
      "source": [
        "df_drug_proportional_augmented.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCknyB_XQMS8"
      },
      "outputs": [],
      "source": [
        "# Saving the proportionally augmented dataframe to CSV\n",
        "df_drug_proportional_augmented.to_csv(\"data/drug_proportional_augmented.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufb5t431SOtr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import itertools\n",
        "\n",
        "def plot_combined_distributions_academic(df1, df2, text_column, label1=\"Original\", label2=\"Augmented\"):\n",
        "    \"\"\"Plots a comparative analysis of text distributions for two datasets.\n",
        "\n",
        "    Parameters:\n",
        "    - df1, df2: DataFrames containing the text data.\n",
        "    - text_column: The name of the column in the DataFrames that contains the text data.\n",
        "    - label1, label2: Labels for the datasets.\n",
        "\n",
        "    Returns:\n",
        "    - A set of three plots: sentence lengths, top 30 word frequencies, and top 30 bigram frequencies.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute sentence lengths\n",
        "    lengths1 = df1[text_column].apply(lambda x: len(x.split()))\n",
        "    lengths2 = df2[text_column].apply(lambda x: len(x.split()))\n",
        "\n",
        "    # Compute word frequencies\n",
        "    words1 = list(itertools.chain(*df1[text_column].str.split().tolist()))\n",
        "    words2 = list(itertools.chain(*df2[text_column].str.split().tolist()))\n",
        "\n",
        "    word_freq1 = dict(Counter(words1).most_common(30))\n",
        "    word_freq2 = dict(Counter(words2).most_common(30))\n",
        "\n",
        "    # Compute bigram frequencies\n",
        "    bigrams1 = [(words1[i], words1[i + 1]) for i in range(len(words1) - 1)]\n",
        "    bigrams2 = [(words2[i], words2[i + 1]) for i in range(len(words2) - 1)]\n",
        "\n",
        "    bigram_freq1 = dict(Counter(bigrams1).most_common(30))\n",
        "    bigram_freq2 = dict(Counter(bigrams2).most_common(30))\n",
        "\n",
        "    # Initialize figure and axes\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "    # Plot sentence length distribution\n",
        "    axs[0].hist(lengths1, bins=50, color='blue', alpha=0.7, label=label1)\n",
        "    axs[0].hist(lengths2, bins=50, color='red', alpha=0.5, label=label2)\n",
        "    axs[0].set_title('Sentence Length Distribution')\n",
        "    axs[0].set_xlabel('Number of Words')\n",
        "    axs[0].set_ylabel('Number of Sentences')\n",
        "    axs[0].legend()\n",
        "\n",
        "    # Plot top 30 word frequencies\n",
        "    top_words = list(set(list(word_freq1.keys()) + list(word_freq2.keys())))\n",
        "    axs[1].bar(top_words, [word_freq1.get(word, 0) for word in top_words], color='blue', alpha=0.7, label=label1)\n",
        "    axs[1].bar(top_words, [word_freq2.get(word, 0) for word in top_words], color='red', alpha=0.5, label=label2, bottom=[word_freq1.get(word, 0) for word in top_words])\n",
        "    axs[1].set_title('Top 30 Word Frequencies')\n",
        "    axs[1].tick_params(axis='x', rotation=90)\n",
        "    axs[1].set_ylabel('Count')\n",
        "    axs[1].legend()\n",
        "\n",
        "    # Plot top 30 bigram frequencies\n",
        "    top_bigrams = list(set(list(bigram_freq1.keys()) + list(bigram_freq2.keys())))\n",
        "    axs[2].bar([' '.join(bigram) for bigram in top_bigrams], [bigram_freq1.get(bigram, 0) for bigram in top_bigrams], color='blue', alpha=0.7, label=label1)\n",
        "    axs[2].bar([' '.join(bigram) for bigram in top_bigrams], [bigram_freq2.get(bigram, 0) for bigram in top_bigrams], color='red', alpha=0.5, label=label2, bottom=[bigram_freq1.get(bigram, 0) for bigram in top_bigrams])\n",
        "    axs[2].set_title('Top 30 Bigram Frequencies')\n",
        "    axs[2].tick_params(axis='x', rotation=90)\n",
        "    axs[2].set_ylabel('Count')\n",
        "    axs[2].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"Comparative_Analysis_Original_vs_Augmented_Text_Distributions.pdf\", format='pdf', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# Invocation\n",
        "plot_combined_distributions_academic(df_drug, df_drug_proportional_augmented, \"original_text\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dQYhJoeJw6l"
      },
      "source": [
        "## Visualizing Text Data: Word Clouds\n",
        "\n",
        "In this section, a Word Cloud analysis is performed on two versions of our text data:\n",
        "\n",
        "1. **Clean Text:** This is the preprocessed text where usernames, hashtags, and stopwords have been removed.\n",
        "2. **Original Text:** This is the unprocessed, raw text from the dataset.\n",
        "\n",
        "The code concatenates all the text entries into large strings, creates `WordCloud` objects, and generates word clouds for each set of text data. Subsequently, it uses matplotlib to plot these word clouds, providing a comparative visual representation of the most frequent words in both versions of the text.\n",
        "\n",
        "This visualization provides insights into the effect of text preprocessing on our data, highlighting the most frequently occurring words in both cleaned and original text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-sGFGT4Xzbd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Load the DataFrames\n",
        "df1 = pd.read_csv('data/covid_cleaned_data.csv')\n",
        "df2 = pd.read_csv('data/mental_health_modified_data.csv')\n",
        "df3 = pd.read_csv('data/drug_cleaned_data.csv')\n",
        "\n",
        "dataframes = [df1, df2, df3]\n",
        "titles = ['COVID Dataset', 'Mental Dataset', 'Drug Dataset']\n",
        "\n",
        "def generate_wordcloud(data, column_name):\n",
        "    long_string = ' '.join(data[column_name].astype(str))\n",
        "    long_string = ' '.join(set(long_string.split(\" \")))  # Remove duplicates\n",
        "    wordcloud = WordCloud(colormap=\"Reds_r\", background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
        "    wordcloud.generate(long_string)\n",
        "    return wordcloud\n",
        "\n",
        "# For the academic style\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "plt.rcParams[\"font.family\"] = \"serif\"\n",
        "\n",
        "fig, axs = plt.subplots(len(dataframes), 2, figsize=(20, 5 * len(dataframes)))\n",
        "\n",
        "# Increase space between subplots for better readability\n",
        "fig.subplots_adjust(hspace=0.6, wspace=0.3)\n",
        "\n",
        "for i, df in enumerate(dataframes):\n",
        "    for column, column_name in zip([0, 1], [\"clean_text_without_usernames_hashtags_or_stopwords\", \"original_text\"]):\n",
        "        wordcloud_obj = generate_wordcloud(df, column_name)\n",
        "        axs[i][column].imshow(wordcloud_obj)\n",
        "        axs[i][column].axis(\"off\")\n",
        "\n",
        "        # Set titles\n",
        "        axs[i][column].set_title(f\"{titles[i]} - {column_name.replace('_', ' ').title()} Column\", fontsize=14)\n",
        "\n",
        "# Set an overall title\n",
        "fig.suptitle('Word Cloud Analysis of Different Datasets', fontsize=18, fontweight='bold', y=1.02)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"wordcloud_analysis_academic.png\", dpi=300)  # Save as high-dpi PNG\n",
        "\n",
        "# Print top N words from each word cloud\n",
        "N = 10\n",
        "\n",
        "for i, df in enumerate(dataframes):\n",
        "    for column, column_name in zip([0, 1], [\"clean_text_without_usernames_hashtags_or_stopwords\", \"original_text\"]):\n",
        "        wordcloud_obj = generate_wordcloud(df, column_name)\n",
        "        word_freqs = wordcloud_obj.words_\n",
        "        sorted_words = sorted(word_freqs.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(f\"\\nTop {N} words for {titles[i]} - {column_name.replace('_', ' ').title()} Column:\")\n",
        "        for word, freq in sorted_words[:N]:\n",
        "            print(f\"{word}: {freq:.2f}\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uyNTRjcjP-d"
      },
      "source": [
        "## TFIDF - SUPPORT VECTOR MACHINE TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbWZMdINZ4Wu"
      },
      "source": [
        "### One hot encoding to int for TFIDF-SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvB_cQnNkvxD"
      },
      "outputs": [],
      "source": [
        "def convert_onehot_to_int(y_data):\n",
        "    \"\"\"\n",
        "    Convert one-hot encoded labels to integer labels.\n",
        "\n",
        "    Parameters:\n",
        "    - y_data: Numpy array with either integer or one-hot encoded labels\n",
        "\n",
        "    Returns:\n",
        "    - Array with integer labels\n",
        "    \"\"\"\n",
        "    return np.argmax(y_data, axis=1) if y_data.ndim > 1 else y_data\n",
        "\n",
        "# Using the function for conversions:\n",
        "y_train_covid_svm = convert_onehot_to_int(y_train_covid)\n",
        "y_test_covid_svm = convert_onehot_to_int(y_test_covid)\n",
        "\n",
        "y_train_mental_svm = convert_onehot_to_int(y_train_mental)\n",
        "y_test_mental_svm = convert_onehot_to_int(y_test_mental)\n",
        "\n",
        "y_train_drug_svm = convert_onehot_to_int(y_train_drug)\n",
        "y_test_drug_svm = convert_onehot_to_int(y_test_drug)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXNYfpbpkpLm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def train_and_evaluate_svm(X, y, dataset_name):\n",
        "    log_dir = f'logs/{dataset_name}_v1'\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "    writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    # Splitting the data using StratifiedKFold\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        # Hyperparameters\n",
        "        tfidf_max_features = 5000\n",
        "        svm_kernel = 'linear'\n",
        "        svm_C = 1\n",
        "        svm_gamma = 'scale'\n",
        "\n",
        "        # TF-IDF\n",
        "        tfidf_vectorizer = TfidfVectorizer(max_features=tfidf_max_features)\n",
        "        X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "        X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "        # SVM\n",
        "        clf = SVC(kernel=svm_kernel, C=svm_C, gamma=svm_gamma)\n",
        "        clf.fit(X_train_tfidf, y_train)\n",
        "        y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "        # Confusion Matrix Visualization\n",
        "        conf_mat = confusion_matrix(y_test, y_pred)\n",
        "        sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "        plt.figure(figsize=(10, 7))\n",
        "        labels = [f'Class {i}' for i in range(np.max(y_train)+1)]\n",
        "        heatmap = sns.heatmap(conf_mat, annot=True, fmt='g', cmap='Blues', xticklabels=labels, yticklabels=labels, cbar=False)\n",
        "        heatmap.tick_params(labelsize=12)\n",
        "        plt.title(f'Confusion Matrix - Fold {fold + 1}', fontsize=15)\n",
        "        plt.xlabel('Predicted labels', fontsize=14)\n",
        "        plt.ylabel('True labels', fontsize=14)\n",
        "\n",
        "        # Save Confusion Matrix Image\n",
        "        figures_dir = 'figures'\n",
        "        if not os.path.exists(figures_dir):\n",
        "            os.makedirs(figures_dir)\n",
        "        fig_filename = os.path.join(figures_dir, f'confusion_matrix_{dataset_name}_fold_{fold + 1}.png')\n",
        "        plt.savefig(fig_filename, format=\"png\", bbox_inches='tight')\n",
        "        print(f\"Confusion matrix for fold {fold + 1} saved at {fig_filename}\")\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format=\"png\", bbox_inches='tight')\n",
        "        buf.seek(0)\n",
        "        image = Image.open(buf).convert(\"RGB\")  # Convert to RGB\n",
        "        image_np = np.array(image)\n",
        "        plt.close()\n",
        "\n",
        "            # Metrics Calculation\n",
        "        is_binary = len(np.unique(np.concatenate([y_train, y_test]))) == 2\n",
        "        if is_binary:\n",
        "            precision = precision_score(y_test, y_pred, pos_label=1)\n",
        "            recall = recall_score(y_test, y_pred, pos_label=1)\n",
        "            f1 = f1_score(y_test, y_pred, pos_label=1)\n",
        "        else:\n",
        "            precision = precision_score(y_test, y_pred, average='weighted')\n",
        "            recall = recall_score(y_test, y_pred, average='micro')\n",
        "            f1 = f1_score(y_test, y_pred, average='micro')\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        # Logging to TensorBoard\n",
        "        with writer.as_default():\n",
        "            tf.summary.image(f\"confusion_matrix_fold_{fold + 1}\", tf.convert_to_tensor(image_np)[tf.newaxis,...], step=fold + 1)\n",
        "            tf.summary.scalar(f'Accuracy_fold_{fold + 1}', accuracy, step=fold + 1)\n",
        "            tf.summary.scalar(f'Precision_fold_{fold + 1}', precision, step=fold + 1)\n",
        "            tf.summary.scalar(f'Recall_fold_{fold + 1}', recall, step=fold + 1)\n",
        "            tf.summary.scalar(f'F1_Score_fold_{fold + 1}', f1, step=fold + 1)\n",
        "            hyperparams_str = \"\\n\".join([f\"{key}: {value}\" for key, value in {\n",
        "                'tfidf_max_features': tfidf_max_features,\n",
        "                'svm_kernel': svm_kernel,\n",
        "                'svm_C': svm_C,\n",
        "                'svm_gamma': svm_gamma\n",
        "            }.items()])\n",
        "            tf.summary.text(f'Hyperparameters_fold_{fold + 1}', hyperparams_str, step=fold + 1)\n",
        "\n",
        "\n",
        "        # Print Classification Report for Each Fold\n",
        "        report = classification_report(y_test, y_pred, target_names=labels)\n",
        "        print(f\"\\nClassification Report for {dataset_name} - Fold {fold + 1}:\\n\", report)\n",
        "\n",
        "# # Combine training and test data, then call the function\n",
        "# train_and_evaluate_svm(np.concatenate([X_train_covid, X_test_covid]), np.concatenate([y_train_covid_svm, y_test_covid_svm]), \"covid\")\n",
        "# train_and_evaluate_svm(np.concatenate([X_train_mental, X_test_mental]), np.concatenate([y_train_mental_svm, y_test_mental_svm]), \"mental_health\")\n",
        "# train_and_evaluate_svm(np.concatenate([X_train_drug, X_test_drug]), np.concatenate([y_train_drug_svm, y_test_drug_svm]), \"drug\")\n",
        "\n",
        "train_and_evaluate_svm(np.concatenate([X_train_aug_drug, X_test_aug_drug]), np.concatenate([y_train_aug_drug_svm, y_test_aug_drug_svm]), \"drug_aug\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyXuNiJsvBVs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def train_and_evaluate_svm(X, y, dataset_name):\n",
        "    log_dir = f'logs/{dataset_name}_countsvm'\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "    writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    # Splitting the data using StratifiedKFold\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        # Tokenization\n",
        "        count_vectorizer = CountVectorizer(max_features=5000)\n",
        "        X_train_counts = count_vectorizer.fit_transform(X_train)\n",
        "        X_test_counts = count_vectorizer.transform(X_test)\n",
        "\n",
        "        # LinearSVC\n",
        "        clf = LinearSVC(C=0.1)\n",
        "        clf.fit(X_train_counts, y_train)\n",
        "        y_pred = clf.predict(X_test_counts)\n",
        "        # Confusion Matrix Visualization\n",
        "        conf_mat = confusion_matrix(y_test, y_pred)\n",
        "        sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "        plt.figure(figsize=(10, 7))\n",
        "        labels = [f'Class {i}' for i in range(np.max(y_train)+1)]\n",
        "        heatmap = sns.heatmap(conf_mat, annot=True, fmt='g', cmap='Blues', xticklabels=labels, yticklabels=labels, cbar=False)\n",
        "        heatmap.tick_params(labelsize=12)\n",
        "        plt.title(f'Confusion Matrix - Fold {fold + 1}', fontsize=15)\n",
        "        plt.xlabel('Predicted labels', fontsize=14)\n",
        "        plt.ylabel('True labels', fontsize=14)\n",
        "\n",
        "        # Save Confusion Matrix Image\n",
        "        figures_dir = 'figures'\n",
        "        if not os.path.exists(figures_dir):\n",
        "            os.makedirs(figures_dir)\n",
        "        fig_filename = os.path.join(figures_dir, f'confusion_matrix_{dataset_name}_fold_{fold + 1}.png')\n",
        "        plt.savefig(fig_filename, format=\"png\", bbox_inches='tight')\n",
        "        print(f\"Confusion matrix for fold {fold + 1} saved at {fig_filename}\")\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format=\"png\", bbox_inches='tight')\n",
        "        buf.seek(0)\n",
        "        image = Image.open(buf).convert(\"RGB\")  # Convert to RGB\n",
        "        image_np = np.array(image)\n",
        "        plt.close()\n",
        "\n",
        "            # Metrics Calculation\n",
        "        is_binary = len(np.unique(np.concatenate([y_train, y_test]))) == 2\n",
        "        if is_binary:\n",
        "            precision = precision_score(y_test, y_pred, pos_label=1)\n",
        "            recall = recall_score(y_test, y_pred, pos_label=1)\n",
        "            f1 = f1_score(y_test, y_pred, pos_label=1)\n",
        "        else:\n",
        "            precision = precision_score(y_test, y_pred, average='weighted')\n",
        "            recall = recall_score(y_test, y_pred, average='micro')\n",
        "            f1 = f1_score(y_test, y_pred, average='micro')\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        # Logging to TensorBoard\n",
        "        with writer.as_default():\n",
        "            tf.summary.image(f\"confusion_matrix_fold_{fold + 1}\", tf.convert_to_tensor(image_np)[tf.newaxis,...], step=fold + 1)\n",
        "            tf.summary.scalar(f'Accuracy_fold_{fold + 1}', accuracy, step=fold + 1)\n",
        "            tf.summary.scalar(f'Precision_fold_{fold + 1}', precision, step=fold + 1)\n",
        "            tf.summary.scalar(f'Recall_fold_{fold + 1}', recall, step=fold + 1)\n",
        "            tf.summary.scalar(f'F1_Score_fold_{fold + 1}', f1, step=fold + 1)\n",
        "\n",
        "        # Print Classification Report for Each Fold\n",
        "        report = classification_report(y_test, y_pred, target_names=labels)\n",
        "        print(f\"\\nClassification Report for {dataset_name} - Fold {fold + 1}:\\n\", report)\n",
        "\n",
        "\n",
        "\n",
        "train_and_evaluate_svm(np.concatenate([X_train_covid, X_test_covid]), np.concatenate([y_train_covid_svm, y_test_covid_svm]), \"covid\")\n",
        "train_and_evaluate_svm(np.concatenate([X_train_mental, X_test_mental]), np.concatenate([y_train_mental_svm, y_test_mental_svm]), \"mental_health\")\n",
        "train_and_evaluate_svm(np.concatenate([X_train_drug, X_test_drug]), np.concatenate([y_train_drug_svm, y_test_drug_svm]), \"drug\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoFVFcmWsJPB"
      },
      "source": [
        "# TrainEval_Word2Vec_BiGRU_LSTM_Attention_EarlyStop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnQYlEKGlIzH"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx926P-IPHsz"
      },
      "outputs": [],
      "source": [
        "!pip install keras-self-attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iDveEUQzFh3"
      },
      "source": [
        "### AttentiveEmoGRU-LSTM Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZw9X-9EC3tW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, Input, Bidirectional, GRU, Dense, Dropout, LSTM\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score\n",
        "from gensim.models import FastText\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "from datetime import datetime\n",
        "from gensim.models import KeyedVectors\n",
        "from keras_self_attention import SeqWeightedAttention\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, Input, Conv1D, Bidirectional, GRU, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_self_attention import SeqWeightedAttention\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from datetime import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import sys\n",
        "import time\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import os\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "import io\n",
        "\n",
        "\n",
        "class Word2VecVectorizer:\n",
        "    def __init__(self, word2vec_path):\n",
        "        self.embedding_dim = 300  # GoogleNews-vectors-negative300.bin has an embedding size of 300\n",
        "        self.model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "\n",
        "    def get_vector(self, word):\n",
        "        try:\n",
        "            return self.model[word]\n",
        "        except KeyError:\n",
        "            return np.zeros(self.embedding_dim)\n",
        "\n",
        "\n",
        "\n",
        "class TextDataPreprocessor:\n",
        "    def __init__(self, max_sequence_length, max_num_words):\n",
        "        self.tokenizer = Tokenizer(num_words=max_num_words)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "\n",
        "    def fit(self, texts):\n",
        "        self.tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    def transform(self, texts):\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        return pad_sequences(sequences, maxlen=self.max_sequence_length)\n",
        "\n",
        "\n",
        "class TextBiGRUModelBuilder:\n",
        "    def __init__(self, sequence_length, num_words, embedding_dim, num_classes, word2vec_vectorizer,tokenizer, dropout_rate=0.1, gru_units=64):\n",
        "        self.config = {\n",
        "            \"sequence_length\": sequence_length,\n",
        "            \"num_words\": num_words,\n",
        "            \"embedding_dim\": embedding_dim,\n",
        "            \"num_classes\": num_classes,\n",
        "            \"dropout_rate\": dropout_rate,\n",
        "            \"gru_units\": gru_units\n",
        "        }\n",
        "        self.word2vec_vectorizer = word2vec_vectorizer\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "\n",
        "    def build_embedding_matrix(self):\n",
        "        embedding_matrix = np.zeros((self.config[\"num_words\"], self.config[\"embedding_dim\"]))\n",
        "        for word, i in self.tokenizer.word_index.items():\n",
        "            if i < self.config[\"num_words\"]:\n",
        "                embedding_vector = self.word2vec_vectorizer.get_vector(word)\n",
        "                # Since get_vector method in Word2VecVectorizer will always return an array (either the embedding or zeros), we don't need to check if it's None.\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "        return embedding_matrix\n",
        "\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "        embedding_matrix = self.build_embedding_matrix()\n",
        "\n",
        "        sequence_input = Input(shape=(self.config[\"sequence_length\"],), dtype='int32')\n",
        "        embedding_layer = Embedding(input_dim=self.config[\"num_words\"],\n",
        "                                    output_dim=self.config[\"embedding_dim\"],\n",
        "                                    weights=[embedding_matrix],\n",
        "                                    input_length=self.config[\"sequence_length\"],\n",
        "                                    trainable=False)(sequence_input)\n",
        "\n",
        "        x = Bidirectional(GRU(self.config[\"gru_units\"], return_sequences=True))(embedding_layer)\n",
        "        x = Dropout(self.config[\"dropout_rate\"])(x)\n",
        "        x = Bidirectional(GRU(self.config[\"gru_units\"], return_sequences=True))(x)\n",
        "        x = Dropout(self.config[\"dropout_rate\"])(x)\n",
        "        x = Bidirectional(LSTM(self.config[\"gru_units\"], return_sequences=True))(x)\n",
        "        x = Dropout(self.config[\"dropout_rate\"])(x)\n",
        "        x, _ = SeqWeightedAttention(return_attention=True)(x)\n",
        "        preds = Dense(self.config[\"num_classes\"], activation='softmax')(x)\n",
        "\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=Adam(),\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "def get_callbacks(fold_num, log_dir):\n",
        "    return [\n",
        "        EarlyStopping(monitor='val_loss', patience=5),\n",
        "        TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
        "        ModelCheckpoint(f'best_model_fold_{fold_num}.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
        "    ]\n",
        "\n",
        "\n",
        "def log_to_tensorboard(log_dir, epoch, confusion_mtx, class_labels, micro_f1, weighted_precision):\n",
        "    figure = plt.figure(figsize=(10, 10))\n",
        "    sns.heatmap(confusion_mtx, annot=True, fmt=\"d\", cmap=plt.cm.Blues, xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close(figure)\n",
        "    buf.seek(0)\n",
        "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    with tf.summary.create_file_writer(log_dir).as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", image, step=epoch)\n",
        "        tf.summary.scalar(\"Micro F1 Score\", micro_f1, step=epoch)\n",
        "        tf.summary.scalar(\"Weighted Precision\", weighted_precision, step=epoch)\n",
        "\n",
        "\n",
        "def train_and_evaluate(X, y, num_classes, dataset_name=\"default_dataset\"):\n",
        "\n",
        "\n",
        "    print(f\"Training for {dataset_name} started...\")\n",
        "    log_dir = f\"logs/{dataset_name}_word2vec_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    max_num_words = 5000\n",
        "    y_labels = np.argmax(y, axis=1)\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    word2vec_path = \"GoogleNews-vectors-negative300.bin\"\n",
        "\n",
        "\n",
        "\n",
        "    for fold_num, (train_index, test_index) in enumerate(skf.split(X, y_labels), start=1):\n",
        "        print(f\"Processing Fold {fold_num} for {dataset_name}...\")\n",
        "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
        "        X_word_seq_train = [text_to_word_sequence(doc) for doc in X_train]\n",
        "        word2vec_vectorizer = Word2VecVectorizer(word2vec_path)\n",
        "\n",
        "        data_preprocessor = TextDataPreprocessor(max_sequence_length=100, max_num_words=max_num_words)\n",
        "        data_preprocessor.fit(X_train)\n",
        "        X_train_prep, X_test_prep = data_preprocessor.transform(X_train), data_preprocessor.transform(X_test)\n",
        "\n",
        "        model_builder = TextBiGRUModelBuilder(sequence_length=100, num_words=max_num_words, embedding_dim=300, num_classes=num_classes,\n",
        "                                    word2vec_vectorizer=word2vec_vectorizer, tokenizer=data_preprocessor.tokenizer, dropout_rate=0.1, gru_units=64)\n",
        "\n",
        "        model = model_builder.build_model()\n",
        "\n",
        "\n",
        "        callbacks = get_callbacks(fold_num, log_dir)\n",
        "\n",
        "        model.fit(X_train_prep, y_train, validation_data=(X_test_prep, y_test), epochs=20, batch_size=128, callbacks=callbacks)\n",
        "\n",
        "        y_pred = model.predict(X_test_prep)\n",
        "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        confusion_mtx = confusion_matrix(y_labels[test_index], y_pred_labels)\n",
        "        micro_f1 = f1_score(y_labels[test_index], y_pred_labels, average='micro')\n",
        "        weighted_precision = precision_score(y_labels[test_index], y_pred_labels, average='weighted')\n",
        "\n",
        "        log_to_tensorboard(log_dir, fold_num, confusion_mtx, list(range(num_classes)), micro_f1, weighted_precision)\n",
        "\n",
        "    print(f\"Training for {dataset_name} completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0ImwzFHZCIj"
      },
      "outputs": [],
      "source": [
        "train_and_evaluate(np.concatenate([X_train_covid, X_test_covid]), np.concatenate([y_train_covid, y_test_covid]), num_classes=3, dataset_name=\"covid_dataset\")\n",
        "train_and_evaluate(np.concatenate([X_train_mental, X_test_mental]), np.concatenate([y_train_mental, y_test_mental]), num_classes=2, dataset_name=\"mental_dataset\")\n",
        "train_and_evaluate(np.concatenate([X_train_drug, X_test_drug]), np.concatenate([y_train_drug, y_test_drug]), num_classes=3, dataset_name=\"drug_dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBXeuduIzWkh"
      },
      "source": [
        "### AttentiveEmoGRU-LSTM Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0aU3EOFzaFE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, Input, Bidirectional, GRU, Dense, Dropout, LSTM\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score\n",
        "from gensim.models import FastText\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "from datetime import datetime\n",
        "from gensim.models import KeyedVectors\n",
        "from keras_self_attention import SeqWeightedAttention\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, Input, Conv1D, Bidirectional, GRU, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_self_attention import SeqWeightedAttention\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from datetime import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import sys\n",
        "import time\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import os\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "import io\n",
        "\n",
        "\n",
        "class FastTextVectorizer:\n",
        "    def __init__(self, fasttext_path):\n",
        "        self.embedding_dim = 300  # Adjust based on the FastText model you're using, most common is 300\n",
        "        self.model = KeyedVectors.load_word2vec_format(fasttext_path, binary=False)  # Set binary to False for .vec format\n",
        "\n",
        "    def get_vector(self, word):\n",
        "        try:\n",
        "            return self.model[word]\n",
        "        except KeyError:\n",
        "            return np.zeros(self.embedding_dim)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TextDataPreprocessor:\n",
        "    def __init__(self, max_sequence_length, max_num_words):\n",
        "        self.tokenizer = Tokenizer(num_words=max_num_words)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "\n",
        "    def fit(self, texts):\n",
        "        self.tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    def transform(self, texts):\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        return pad_sequences(sequences, maxlen=self.max_sequence_length)\n",
        "\n",
        "\n",
        "class TextBiGRUModelBuilder:\n",
        "    def __init__(self, sequence_length, num_words, embedding_dim, num_classes, fasttext_vectorizer,tokenizer, dropout_rate=0.1, gru_units=64):\n",
        "        self.config = {\n",
        "            \"sequence_length\": sequence_length,\n",
        "            \"num_words\": num_words,\n",
        "            \"embedding_dim\": embedding_dim,\n",
        "            \"num_classes\": num_classes,\n",
        "            \"dropout_rate\": dropout_rate,\n",
        "            \"gru_units\": gru_units\n",
        "        }\n",
        "        self.fasttext_vectorizer = fasttext_vectorizer\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "\n",
        "    def build_embedding_matrix(self):\n",
        "        embedding_matrix = np.zeros((self.config[\"num_words\"], self.config[\"embedding_dim\"]))\n",
        "        for word, i in self.tokenizer.word_index.items():\n",
        "            if i < self.config[\"num_words\"]:\n",
        "                embedding_vector = self.fasttext_vectorizer.get_vector(word)\n",
        "                # Since get_vector method in Word2VecVectorizer will always return an array (either the embedding or zeros), we don't need to check if it's None.\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "        return embedding_matrix\n",
        "\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "        embedding_matrix = self.build_embedding_matrix()\n",
        "\n",
        "        sequence_input = Input(shape=(self.config[\"sequence_length\"],), dtype='int32')\n",
        "        embedding_layer = Embedding(input_dim=self.config[\"num_words\"],\n",
        "                                    output_dim=self.config[\"embedding_dim\"],\n",
        "                                    weights=[embedding_matrix],\n",
        "                                    input_length=self.config[\"sequence_length\"],\n",
        "                                    trainable=False)(sequence_input)\n",
        "\n",
        "        x = Bidirectional(GRU(self.config[\"gru_units\"], return_sequences=True))(embedding_layer)\n",
        "        x = Dropout(self.config[\"dropout_rate\"])(x)\n",
        "        x = Bidirectional(GRU(self.config[\"gru_units\"], return_sequences=True))(x)\n",
        "        x = Dropout(self.config[\"dropout_rate\"])(x)\n",
        "        x = Bidirectional(LSTM(self.config[\"gru_units\"], return_sequences=True))(x)\n",
        "        x = Dropout(self.config[\"dropout_rate\"])(x)\n",
        "        x, _ = SeqWeightedAttention(return_attention=True)(x)\n",
        "        preds = Dense(self.config[\"num_classes\"], activation='softmax')(x)\n",
        "\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=Adam(),\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "def get_callbacks(fold_num, log_dir):\n",
        "    return [\n",
        "        EarlyStopping(monitor='val_loss', patience=5),\n",
        "        TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
        "        ModelCheckpoint(f'best_model_fold_{fold_num}.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
        "    ]\n",
        "\n",
        "\n",
        "def log_to_tensorboard(log_dir, epoch, confusion_mtx, class_labels, micro_f1, weighted_precision):\n",
        "    figure = plt.figure(figsize=(10, 10))\n",
        "    sns.heatmap(confusion_mtx, annot=True, fmt=\"d\", cmap=plt.cm.Blues, xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close(figure)\n",
        "    buf.seek(0)\n",
        "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    with tf.summary.create_file_writer(log_dir).as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", image, step=epoch)\n",
        "        tf.summary.scalar(\"Micro F1 Score\", micro_f1, step=epoch)\n",
        "        tf.summary.scalar(\"Weighted Precision\", weighted_precision, step=epoch)\n",
        "\n",
        "\n",
        "def train_and_evaluate(X, y, num_classes, dataset_name=\"default_dataset\"):\n",
        "\n",
        "\n",
        "    print(f\"Training for {dataset_name} started...\")\n",
        "    log_dir = f\"logs/{dataset_name}_fasttext_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    max_num_words = 5000\n",
        "    y_labels = np.argmax(y, axis=1)\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fasttext_path = \"cc.en.300.vec\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for fold_num, (train_index, test_index) in enumerate(skf.split(X, y_labels), start=1):\n",
        "        print(f\"Processing Fold {fold_num} for {dataset_name}...fasttext\")\n",
        "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
        "        X_word_seq_train = [text_to_word_sequence(doc) for doc in X_train]\n",
        "        fasttext_vectorizer = FastTextVectorizer(fasttext_path)\n",
        "\n",
        "\n",
        "        data_preprocessor = TextDataPreprocessor(max_sequence_length=100, max_num_words=max_num_words)\n",
        "        data_preprocessor.fit(X_train)\n",
        "        X_train_prep, X_test_prep = data_preprocessor.transform(X_train), data_preprocessor.transform(X_test)\n",
        "\n",
        "        model_builder = TextBiGRUModelBuilder(sequence_length=100, num_words=max_num_words, embedding_dim=300, num_classes=num_classes,\n",
        "                                    fasttext_vectorizer=fasttext_vectorizer, tokenizer=data_preprocessor.tokenizer, dropout_rate=0.1, gru_units=64)\n",
        "\n",
        "        model = model_builder.build_model()\n",
        "\n",
        "\n",
        "        callbacks = get_callbacks(fold_num, log_dir)\n",
        "\n",
        "        model.fit(X_train_prep, y_train, validation_data=(X_test_prep, y_test), epochs=20, batch_size=128, callbacks=callbacks)\n",
        "\n",
        "        y_pred = model.predict(X_test_prep)\n",
        "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        confusion_mtx = confusion_matrix(y_labels[test_index], y_pred_labels)\n",
        "        micro_f1 = f1_score(y_labels[test_index], y_pred_labels, average='micro')\n",
        "        weighted_precision = precision_score(y_labels[test_index], y_pred_labels, average='weighted')\n",
        "\n",
        "        log_to_tensorboard(log_dir, fold_num, confusion_mtx, list(range(num_classes)), micro_f1, weighted_precision)\n",
        "\n",
        "    print(f\"Training for {dataset_name} completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mJeUaOh0FYW"
      },
      "outputs": [],
      "source": [
        "train_and_evaluate(np.concatenate([X_train_covid, X_test_covid]), np.concatenate([y_train_covid, y_test_covid]), num_classes=3, dataset_name=\"covid_dataset\")\n",
        "train_and_evaluate(np.concatenate([X_train_mental, X_test_mental]), np.concatenate([y_train_mental, y_test_mental]), num_classes=2, dataset_name=\"mental_dataset\")\n",
        "train_and_evaluate(np.concatenate([X_train_drug, X_test_drug]), np.concatenate([y_train_drug, y_test_drug]), num_classes=3, dataset_name=\"drug_dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G37Pvpb05Ywv"
      },
      "source": [
        "### AttentiveEmoGRU-LSTM GLOVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gvh6DKIt5ac_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, Input, Bidirectional, GRU, Dense, Dropout, LSTM\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score\n",
        "from gensim.models import FastText\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "from datetime import datetime\n",
        "\n",
        "class GloveVectorizer:\n",
        "    def __init__(self, glove_path, embedding_dim=100):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings_index = self._load_glove(glove_path)\n",
        "\n",
        "    def _load_glove(self, glove_path):\n",
        "        embeddings_index = {}\n",
        "        with open(glove_path, 'r', encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                coefs = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_index[word] = coefs\n",
        "        return embeddings_index\n",
        "\n",
        "    def get_vector(self, word):\n",
        "        return self.embeddings_index.get(word, np.zeros(self.embedding_dim))\n",
        "\n",
        "\n",
        "class TextDataPreprocessor:\n",
        "    def __init__(self, max_sequence_length, max_num_words):\n",
        "        self.tokenizer = Tokenizer(num_words=max_num_words)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "\n",
        "    def fit(self, texts):\n",
        "        self.tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    def transform(self, texts):\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        return pad_sequences(sequences, maxlen=self.max_sequence_length)\n",
        "\n",
        "\n",
        "class TextBiGRUModelBuilder:\n",
        "    def __init__(self, sequence_length, num_words, embedding_dim, num_classes, glove_vectorizer,tokenizer, dropout_rate=0.1, gru_units=64):\n",
        "        self.config = {\n",
        "            \"sequence_length\": sequence_length,\n",
        "            \"num_words\": num_words,\n",
        "            \"embedding_dim\": embedding_dim,\n",
        "            \"num_classes\": num_classes,\n",
        "            \"dropout_rate\": dropout_rate,\n",
        "            \"gru_units\": gru_units\n",
        "        }\n",
        "        self.glove_vectorizer = glove_vectorizer\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "\n",
        "    def build_embedding_matrix(self):\n",
        "        embedding_matrix = np.zeros((self.config[\"num_words\"], self.config[\"embedding_dim\"]))\n",
        "        for word, i in self.tokenizer.word_index.items():\n",
        "            if i < self.config[\"num_words\"]:\n",
        "                embedding_vector = self.glove_vectorizer.get_vector(word)\n",
        "                if embedding_vector is not None:\n",
        "                    embedding_matrix[i] = embedding_vector\n",
        "        return embedding_matrix\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "        embedding_matrix = self.build_embedding_matrix()\n",
        "\n",
        "        sequence_input = Input(shape=(self.config[\"sequence_length\"],), dtype='int32')\n",
        "        embedding_layer = Embedding(input_dim=self.config[\"num_words\"],\n",
        "                                    output_dim=self.config[\"embedding_dim\"],\n",
        "                                    weights=[embedding_matrix],\n",
        "                                    input_length=self.config[\"sequence_length\"],\n",
        "                                    trainable=False)(sequence_input)\n",
        "\n",
        "        x = Bidirectional(GRU(self.config[\"gru_units\"], return_sequences=True))(embedding_layer)\n",
        "        x = Dropout(self.config[\"dropout_rate\"])(x)\n",
        "        x = Bidirectional(GRU(self.config[\"gru_units\"], return_sequences=True))(x)\n",
        "        x = Dropout(self.config[\"dropout_rate\"])(x)\n",
        "        x = Bidirectional(LSTM(self.config[\"gru_units\"], return_sequences=True))(x)\n",
        "        x = Dropout(self.config[\"dropout_rate\"])(x)\n",
        "        x, _ = SeqWeightedAttention(return_attention=True)(x)\n",
        "        preds = Dense(self.config[\"num_classes\"], activation='softmax')(x)\n",
        "\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=Adam(),\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "def get_callbacks(fold_num, log_dir):\n",
        "    return [\n",
        "        EarlyStopping(monitor='val_loss', patience=5),\n",
        "        TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
        "        ModelCheckpoint(f'best_model_fold_{fold_num}.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
        "    ]\n",
        "\n",
        "\n",
        "def log_to_tensorboard(log_dir, epoch, confusion_mtx, class_labels, micro_f1, weighted_precision):\n",
        "    figure = plt.figure(figsize=(10, 10))\n",
        "    sns.heatmap(confusion_mtx, annot=True, fmt=\"d\", cmap=plt.cm.Blues, xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close(figure)\n",
        "    buf.seek(0)\n",
        "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    with tf.summary.create_file_writer(log_dir).as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", image, step=epoch)\n",
        "        tf.summary.scalar(\"Micro F1 Score\", micro_f1, step=epoch)\n",
        "        tf.summary.scalar(\"Weighted Precision\", weighted_precision, step=epoch)\n",
        "\n",
        "\n",
        "def train_and_evaluate(X, y, num_classes, dataset_name=\"default_dataset\"):\n",
        "\n",
        "\n",
        "    print(f\"Training for {dataset_name} started...\")\n",
        "    log_dir = f\"logs/{dataset_name}_glove_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    max_num_words = 5000\n",
        "    y_labels = np.argmax(y, axis=1)\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    glove_path = \"glove.6B.50d.txt\"\n",
        "\n",
        "    for fold_num, (train_index, test_index) in enumerate(skf.split(X, y_labels), start=1):\n",
        "        print(f\"Processing Fold {fold_num} for {dataset_name}...\")\n",
        "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
        "        X_word_seq_train = [text_to_word_sequence(doc) for doc in X_train]\n",
        "        glove_vectorizer = GloveVectorizer(glove_path, embedding_dim=50)\n",
        "\n",
        "        data_preprocessor = TextDataPreprocessor(max_sequence_length=100, max_num_words=max_num_words)\n",
        "        data_preprocessor.fit(X_train)\n",
        "        X_train_prep, X_test_prep = data_preprocessor.transform(X_train), data_preprocessor.transform(X_test)\n",
        "\n",
        "        model_builder = TextBiGRUModelBuilder(sequence_length=100, num_words=max_num_words, embedding_dim=50, num_classes=num_classes,\n",
        "                                    glove_vectorizer=glove_vectorizer, tokenizer=data_preprocessor.tokenizer, dropout_rate=0.1, gru_units=64)\n",
        "\n",
        "        model = model_builder.build_model()\n",
        "\n",
        "\n",
        "        callbacks = get_callbacks(fold_num, log_dir)\n",
        "\n",
        "        model.fit(X_train_prep, y_train, validation_data=(X_test_prep, y_test), epochs=20, batch_size=128, callbacks=callbacks)\n",
        "\n",
        "        y_pred = model.predict(X_test_prep)\n",
        "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        confusion_mtx = confusion_matrix(y_labels[test_index], y_pred_labels)\n",
        "        micro_f1 = f1_score(y_labels[test_index], y_pred_labels, average='micro')\n",
        "        weighted_precision = precision_score(y_labels[test_index], y_pred_labels, average='weighted')\n",
        "\n",
        "        log_to_tensorboard(log_dir, fold_num, confusion_mtx, list(range(num_classes)), micro_f1, weighted_precision)\n",
        "\n",
        "    print(f\"Training for {dataset_name} completed!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8cOENs3HSzB"
      },
      "outputs": [],
      "source": [
        "train_and_evaluate(np.concatenate([X_train_covid, X_test_covid]), np.concatenate([y_train_covid, y_test_covid]), num_classes=3, dataset_name=\"covid_dataset\")\n",
        "train_and_evaluate(np.concatenate([X_train_mental, X_test_mental]), np.concatenate([y_train_mental, y_test_mental]), num_classes=2, dataset_name=\"mental_dataset\")\n",
        "train_and_evaluate(np.concatenate([X_train_drug, X_test_drug]), np.concatenate([y_train_drug, y_test_drug]), num_classes=3, dataset_name=\"drug_dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBqjqT5uoMJ6"
      },
      "source": [
        "## FINE_TUNING_TEST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXybure4ii7U"
      },
      "source": [
        "# Shared Layer RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Prepare data\n",
        "data = {\n",
        "    'Fold': ['F1', 'F2', 'F3', 'F4', 'F5'] * 9,\n",
        "    'Embedding': ['Word2Vec'] * 5 + ['FastText'] * 5 + ['Glove'] * 5 +\n",
        "                 ['Word2Vec'] * 5 + ['FastText'] * 5 + ['Glove'] * 5 +\n",
        "                 ['Word2Vec'] * 5 + ['FastText'] * 5 + ['Glove'] * 5,\n",
        "    'Dataset': ['COVID'] * 15 + ['Mental'] * 15 + ['Drug Review'] * 15,\n",
        "    'Micro F1': [\n",
        "        0.796, 0.806, 0.806, 0.787, 0.807,\n",
        "        0.773, 0.790, 0.793, 0.778, 0.778,\n",
        "        0.731, 0.750, 0.757, 0.733, 0.748,\n",
        "        0.909, 0.907, 0.900, 0.911, 0.909,\n",
        "        0.912, 0.909, 0.909, 0.909, 0.909,\n",
        "        0.896, 0.898, 0.898, 0.876, 0.902,\n",
        "        0.693, 0.716, 0.701, 0.710, 0.713,\n",
        "        0.729, 0.715, 0.695, 0.706, 0.706,\n",
        "        0.715, 0.711, 0.721, 0.688, 0.678\n",
        "    ],\n",
        "    'Precision': [\n",
        "        0.797, 0.805, 0.805, 0.786, 0.809,\n",
        "        0.773, 0.789, 0.794, 0.779, 0.781,\n",
        "        0.741, 0.751, 0.758, 0.734, 0.748,\n",
        "        0.909, 0.907, 0.900, 0.911, 0.914,\n",
        "        0.913, 0.909, 0.910, 0.910, 0.909,\n",
        "        0.896, 0.898, 0.899, 0.905, 0.902,\n",
        "        0.643, 0.662, 0.673, 0.671, 0.677,\n",
        "        0.682, 0.683, 0.631, 0.633, 0.632,\n",
        "        0.719, 0.660, 0.651, 0.618, 0.660\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df['Fold'] = df['Fold'].str[1:].astype(int)\n",
        "\n",
        "plt.rcParams['axes.titlepad'] = 20\n",
        "plt.rcParams['axes.labelpad'] = 10\n",
        "\n",
        "# Contrasting color palette\n",
        "palette = [\"#E63946\", \"#1D3557\", \"#2A9D8F\"]  # Red, Dark Blue, and Teal\n",
        "\n",
        "# Define line styles\n",
        "line_styles = {\n",
        "    'Word2Vec': '-',\n",
        "    'FastText': '--',\n",
        "    'Glove': '-.'\n",
        "}\n",
        "\n",
        "# Lists for datasets and metrics\n",
        "datasets = ['COVID', 'Mental', 'Drug Review']\n",
        "metrics = ['Micro F1', 'Precision']\n",
        "\n",
        "# Create a 2x3 grid of subplots with a tighter figsize\n",
        "fig, axes = plt.subplots(2, 3, figsize=(24, 11))\n",
        "\n",
        "for j, dataset in enumerate(datasets):\n",
        "    for i, metric in enumerate(metrics):\n",
        "        ax = axes[i, j]\n",
        "        subset_df = df[df['Dataset'] == dataset]\n",
        "\n",
        "        # Loop over embedding types for polynomial regression\n",
        "        for embedding in ['Word2Vec', 'FastText', 'Glove']:\n",
        "            subset_embedding_df = subset_df[subset_df['Embedding'] == embedding]\n",
        "            sns.lineplot(data=subset_embedding_df, x='Fold', y=metric, ax=ax,\n",
        "                         label=embedding, color=palette[list(['Word2Vec', 'FastText', 'Glove']).index(embedding)],\n",
        "                         linestyle=line_styles[embedding], linewidth=2.5)\n",
        "\n",
        "            # Annotate data points\n",
        "            prev_y = None\n",
        "            for x, y in zip(subset_embedding_df['Fold'], subset_embedding_df[metric]):\n",
        "                offset = 0.002\n",
        "                if prev_y and y < prev_y:\n",
        "                    position_y = y - offset\n",
        "                else:\n",
        "                    position_y = y + offset\n",
        "\n",
        "                ax.text(x, position_y, f'{y:.3f}', color=palette[list(['Word2Vec', 'FastText', 'Glove']).index(embedding)], ha='center')\n",
        "                prev_y = y\n",
        "\n",
        "        # Common configurations\n",
        "        ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "        ax.set_title(f'{dataset} - {metric}', fontsize=14)\n",
        "\n",
        "        # Hide individual legends\n",
        "        ax.legend().set_visible(False)\n",
        "\n",
        "        # Adjust labels\n",
        "        ax.set_xlabel(ax.get_xlabel(), fontsize=12)\n",
        "        ax.set_ylabel(ax.get_ylabel(), fontsize=12)\n",
        "\n",
        "        # Adjust ticks\n",
        "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
        "\n",
        "lines, labels = ax.get_legend_handles_labels()\n",
        "fig.legend(lines[:3], labels[:3], loc='lower center', bbox_to_anchor=(0.5, 1.05), fontsize=12, title='Embedding Type', ncol=3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "huw-fi6oJ5U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# New BERT data\n",
        "data = {\n",
        "    'Fold': ['F1', 'F2', 'F3', 'F4', 'F5'] * 3,\n",
        "    'Dataset': ['COVID Dataset'] * 5 + ['Mental Dataset'] * 5 + ['Drug Dataset'] * 5,\n",
        "    'Micro F1': [0.8385, 0.9066, 0.9430, 0.9539, 0.9578,\n",
        "                 0.9410, 0.9778, 0.9845, 0.9852, 0.9859,\n",
        "                 0.7150, 0.7367, 0.8939, 0.9110, 0.9318],\n",
        "    'Weighted Precision': [0.8383, 0.9071, 0.9430, 0.9541, 0.9579,\n",
        "                           0.9415, 0.9780, 0.9845, 0.9852, 0.9859,\n",
        "                           0.5966, 0.7013, 0.8896, 0.9102, 0.9309]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df['Fold'] = df['Fold'].str[1:].astype(int)\n",
        "\n",
        "# Define chart styles\n",
        "plt.rcParams['axes.titlepad'] = 20\n",
        "plt.rcParams['axes.labelpad'] = 10\n",
        "palette = [\"#E63946\", \"#1D3557\", \"#2A9D8F\"]  # Red, Dark Blue, and Teal\n",
        "\n",
        "# Lists for metrics\n",
        "metrics = ['Micro F1', 'Weighted Precision']\n",
        "\n",
        "# Create a 1x2 grid of subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(24, 6))\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    ax = axes[i]\n",
        "\n",
        "    sns.lineplot(data=df, x='Fold', y=metric, hue='Dataset', ax=ax, palette=palette, linewidth=2.5)\n",
        "\n",
        "    # Annotate data points\n",
        "    for idx, dataset in enumerate(['COVID Dataset', 'Mental Dataset', 'Drug Review Dataset']):\n",
        "        subset_df = df[df['Dataset'] == dataset]\n",
        "        prev_y = None\n",
        "        for x, y in zip(subset_df['Fold'], subset_df[metric]):\n",
        "            offset = 0.002\n",
        "            if prev_y and y < prev_y:\n",
        "                position_y = y - offset\n",
        "            else:\n",
        "                position_y = y + offset\n",
        "            ax.text(x, position_y, f'{y:.3f}', color=palette[idx], ha='center', fontsize=9)\n",
        "            prev_y = y\n",
        "\n",
        "    # Common configurations\n",
        "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    ax.set_title(f'{metric}', fontsize=14)\n",
        "\n",
        "    # Adjust labels\n",
        "    ax.set_xlabel(ax.get_xlabel(), fontsize=12)\n",
        "    ax.set_ylabel(ax.get_ylabel(), fontsize=12)\n",
        "\n",
        "    # Adjust ticks\n",
        "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
        "    ax.legend(title='Dataset', fontsize=10, title_fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RyDyjAgtF2DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "statistics = {\n",
        "    \"Mean\": [],\n",
        "    \"Median\": [],\n",
        "    \"Standard Deviation\": [],\n",
        "    \"Variance\": [],\n",
        "    \"Range\": [],\n",
        "    \"IQR\": [],\n",
        "    \"Skewness\": [],\n",
        "    \"Kurtosis\": [],\n",
        "    \"25th Percentile\": [],\n",
        "    \"75th Percentile\": [],\n",
        "    \"Coefficient of Variation\": [],\n",
        "    \"Mode\": [],\n",
        "    \"Count\": []\n",
        "}\n",
        "\n",
        "embeddings = ['Word2Vec', 'FastText', 'Glove']\n",
        "datasets = ['COVID', 'Mental', 'Drug Review']\n",
        "metrics = ['Micro F1', 'Precision']\n",
        "\n",
        "latex_output = \"\"\n",
        "\n",
        "for metric in metrics:\n",
        "    for dataset in datasets:\n",
        "        for embedding in embeddings:\n",
        "            subset = df[(df['Embedding'] == embedding) & (df['Dataset'] == dataset)][metric]\n",
        "\n",
        "            statistics[\"Mean\"].append(subset.mean())\n",
        "            statistics[\"Median\"].append(subset.median())\n",
        "            statistics[\"Standard Deviation\"].append(subset.std())\n",
        "            statistics[\"Variance\"].append(subset.var())\n",
        "            statistics[\"Range\"].append(subset.max() - subset.min())\n",
        "            statistics[\"IQR\"].append(subset.quantile(0.75) - subset.quantile(0.25))\n",
        "            statistics[\"Skewness\"].append(subset.skew())\n",
        "            statistics[\"Kurtosis\"].append(subset.kurt())\n",
        "            statistics[\"25th Percentile\"].append(subset.quantile(0.25))\n",
        "            statistics[\"75th Percentile\"].append(subset.quantile(0.75))\n",
        "            statistics[\"Coefficient of Variation\"].append((subset.std() / subset.mean()) * 100)\n",
        "            statistics[\"Mode\"].append(subset.mode().iloc[0])\n",
        "            statistics[\"Count\"].append(subset.count())\n",
        "\n",
        "            # Generate LaTeX table entries for each dataset and embedding type\n",
        "            latex_output += f\"\\\\textbf{{{dataset} - {embedding} - {metric}}} \\\\\\\\ \\\\hline \\n\"\n",
        "            for stat, value in statistics.items():\n",
        "                latex_output += f\"{stat} & {value[-1]:.3f} \\\\\\\\ \\n\"\n",
        "\n",
        "            latex_output += \"\\\\hline \\n\"\n",
        "\n",
        "# Print the LaTeX table entries\n",
        "print(latex_output)"
      ],
      "metadata": {
        "id": "BL2KsCsnetKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming your dataframe df and other variables are already loaded\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(14, 20))\n",
        "\n",
        "# 1. Heatmap\n",
        "heatmap_data = {\n",
        "    \"Word2Vec\": [],\n",
        "    \"FastText\": [],\n",
        "    \"Glove\": []\n",
        "}\n",
        "\n",
        "for dataset in datasets:\n",
        "    for embedding in embeddings:\n",
        "        subset = df[(df['Embedding'] == embedding) & (df['Dataset'] == dataset)]['Micro F1']\n",
        "        heatmap_data[embedding].append(subset.mean())\n",
        "\n",
        "heatmap_df = pd.DataFrame(heatmap_data, index=datasets)\n",
        "sns.heatmap(heatmap_df, cmap=\"YlGnBu\", annot=True, ax=axes[0])\n",
        "axes[0].set_title('Mean Micro F1 Score')\n",
        "\n",
        "# 2. Bar Chart\n",
        "dataset_of_interest = \"COVID\"  # or 'Mental' or 'Drug Review'\n",
        "metric_of_interest = \"Micro F1\"\n",
        "\n",
        "values = [df[(df['Embedding'] == embedding) & (df['Dataset'] == dataset_of_interest)][metric_of_interest].mean() for embedding in embeddings]\n",
        "sns.barplot(x=embeddings, y=values, ax=axes[1])\n",
        "axes[1].set_title(f'Mean {metric_of_interest} for {dataset_of_interest}')\n",
        "axes[1].set_ylabel(metric_of_interest)\n",
        "\n",
        "# 3. Boxplot\n",
        "sns.boxplot(data=df, x=\"Embedding\", y=\"Micro F1\", hue=\"Dataset\", ax=axes[2])\n",
        "axes[2].set_title('Distribution of Micro F1 across Embeddings and Datasets')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "De59isuigB1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pip"
      ],
      "metadata": {
        "id": "e7WPvT9YY4vd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-hOhmdCikm8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, f1_score, confusion_matrix\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score\n",
        "import tensorflow as tf\n",
        "import io\n",
        "from transformers import TFRobertaModel, RobertaTokenizer\n",
        "\n",
        "\n",
        "\n",
        "roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_model(num_classes):\n",
        "    input_ids = Input(shape=(100,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = Input(shape=(100,), dtype=tf.int32, name='attention_mask')\n",
        "    roberta_output = roberta_model([input_ids, attention_mask])[0]\n",
        "    pooled_output = tf.keras.layers.GlobalAveragePooling1D()(roberta_output)\n",
        "    predictions = Dense(num_classes, activation='softmax', name='output')(pooled_output)\n",
        "    model = Model(inputs=[input_ids, attention_mask], outputs=predictions)\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_callbacks(fold_num, log_dir):\n",
        "    return [\n",
        "        EarlyStopping(monitor='val_loss', patience=5),\n",
        "        TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
        "        ModelCheckpoint(f'best_model_fold_{fold_num}.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
        "    ]\n",
        "\n",
        "def log_to_tensorboard(log_dir, epoch, confusion_mtx, class_labels, micro_f1, weighted_precision):\n",
        "    figure = plt.figure(figsize=(10, 10))\n",
        "    sns.heatmap(confusion_mtx, annot=True, fmt=\"d\", cmap=plt.cm.Blues, xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close(figure)\n",
        "    buf.seek(0)\n",
        "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    with tf.summary.create_file_writer(log_dir).as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", image, step=epoch)\n",
        "        tf.summary.scalar(\"Micro F1 Score\", micro_f1, step=epoch)\n",
        "        tf.summary.scalar(\"Weighted Precision\", weighted_precision, step=epoch)\n",
        "\n",
        "def train_and_evaluate(X, y, num_classes, dataset_name=\"default_dataset\"):\n",
        "\n",
        "\n",
        "    print(f\"Training for {dataset_name} started...\")\n",
        "    log_dir = f\"logs/{dataset_name}_roberta\"\n",
        "    y_labels = np.argmax(y, axis=1)\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold_num, (train_index, test_index) in enumerate(skf.split(X, y_labels), start=1):\n",
        "        print(f\"Processing Fold {fold_num} for {dataset_name}...\")\n",
        "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
        "        X_train_text = X_train.tolist()\n",
        "        X_test_text = X_test.tolist()\n",
        "\n",
        "        train_encodings = tokenizer(X_train_text, truncation=True, padding='max_length', max_length=100, return_tensors='tf')\n",
        "        test_encodings = tokenizer(X_test_text, truncation=True, padding='max_length', max_length=100, return_tensors='tf')\n",
        "\n",
        "\n",
        "        X_train_bert = [np.array(train_encodings['input_ids']), np.array(train_encodings['attention_mask'])]\n",
        "        X_test_bert = [np.array(test_encodings['input_ids']), np.array(test_encodings['attention_mask'])]\n",
        "\n",
        "        callbacks = get_callbacks(fold_num, log_dir)\n",
        "        model = create_model(num_classes)\n",
        "\n",
        "        model.fit(X_train_bert, y_train, batch_size=64, epochs=3, validation_split=0.2, callbacks=callbacks)\n",
        "\n",
        "        y_pred = model.predict(X_test_bert)\n",
        "\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "        confusion_mtx = confusion_matrix(y_test_classes, y_pred_classes)\n",
        "        micro_f1 = f1_score(y_test_classes, y_pred_classes, average='micro')\n",
        "        weighted_precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
        "\n",
        "        log_to_tensorboard(log_dir, fold_num, confusion_mtx, list(range(num_classes)), micro_f1, weighted_precision)\n",
        "\n",
        "    print(f\"Training for {dataset_name} completed!\")\n",
        "\n",
        "train_and_evaluate(np.concatenate([X_train_covid, X_test_covid]), np.concatenate([y_train_covid, y_test_covid]), num_classes=3, dataset_name=\"covid_dataset\")\n",
        "train_and_evaluate(np.concatenate([X_train_mental, X_test_mental]), np.concatenate([y_train_mental, y_test_mental]), num_classes=2, dataset_name=\"mental_dataset\")\n",
        "train_and_evaluate(np.concatenate([X_train_drug, X_test_drug]), np.concatenate([y_train_drug, y_test_drug]), num_classes=3, dataset_name=\"drug_dataset\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9iV33Nj6WZh"
      },
      "outputs": [],
      "source": [
        "# Prepare the data for each domain\n",
        "X_train_text_domain1 = X_train_domain1.tolist()\n",
        "X_train_text_domain2 = X_train_domain2.tolist()\n",
        "\n",
        "train_encodings_domain1 = tokenizer(X_train_text_domain1, truncation=True, padding='max_length', max_length=100)\n",
        "train_encodings_domain2 = tokenizer(X_train_text_domain2, truncation=True, padding='max_length', max_length=100)\n",
        "\n",
        "X_train_bert_domain1 = [np.array(train_encodings_domain1['input_ids']), np.array(train_encodings_domain1['attention_mask'])]\n",
        "X_train_bert_domain2 = [np.array(train_encodings_domain2['input_ids']), np.array(train_encodings_domain2['attention_mask'])]\n",
        "\n",
        "# Train the model on each domain\n",
        "for epoch in range(num_epochs):\n",
        "    # Train on domain1\n",
        "    history1 = new_model.fit(X_train_bert_domain1, y_train_domain1, batch_size=64, epochs=1, validation_split=0.2, callbacks=callbacks_list, shuffle=True)\n",
        "    # Update the shared layer to use domain2\n",
        "    new_model.get_layer('shared_layer').domain_index = 1\n",
        "    # Train on domain2\n",
        "    history2 = new_model.fit(X_train_bert_domain2, y_train_domain2, batch_size=64, epochs=1, validation_split=0.2, callbacks=callbacks_list, shuffle=True)\n",
        "    # Update the shared layer to use domain1 for the next epoch\n",
        "    new_model.get_layer('shared_layer').domain_index = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import TFDistilBertModel, DistilBertTokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt\n",
        "import datetime\n",
        "\n",
        "# Load the DistilBERT model and tokenizer\n",
        "distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Assuming you've already loaded and processed your data, else this part would error out:\n",
        "X_train_text = X_train.tolist()\n",
        "X_test_text = X_test.tolist()\n",
        "\n",
        "# Tokenizing\n",
        "train_encodings = tokenizer(X_train_text, truncation=True, padding='max_length', max_length=100)\n",
        "test_encodings = tokenizer(X_test_text, truncation=True, padding='max_length', max_length=100)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_train_distilbert = [np.array(train_encodings['input_ids']), np.array(train_encodings['attention_mask'])]\n",
        "X_test_distilbert = [np.array(test_encodings['input_ids']), np.array(test_encodings['attention_mask'])]\n",
        "\n",
        "# Common layers function\n",
        "def common_layers(inputs):\n",
        "    x = Dense(256, activation='relu')(inputs)\n",
        "    return Dropout(0.5)(x)\n",
        "\n",
        "# Define the shared layer\n",
        "class SharedLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, number_of_domains):\n",
        "        super(SharedLayer, self).__init__()\n",
        "        self.domain_specific_layers = [Dense(3, activation='softmax') for _ in range(number_of_domains)]\n",
        "\n",
        "    def call(self, inputs, domain_index=None):\n",
        "        x = common_layers(inputs)\n",
        "        return self.domain_specific_layers[domain_index](x)\n",
        "\n",
        "# Define the input layers\n",
        "input_ids = Input(shape=(100,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = Input(shape=(100,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "# Get the DistilBERT model output\n",
        "distilbert_output = distilbert_model([input_ids, attention_mask])[0]\n",
        "pooled_output = tf.keras.layers.GlobalAveragePooling1D()(distilbert_output)  # Pooling strategy for DistilBERT\n",
        "\n",
        "# Use shared layers\n",
        "number_of_domains = 2\n",
        "shared_layer = SharedLayer(number_of_domains)\n",
        "predictions = shared_layer(pooled_output, 0)\n",
        "\n",
        "# Model definition using dynamic shared layer\n",
        "class DynamicSharedModel(Model):\n",
        "    def __init__(self, base_model, shared_layer, **kwargs):\n",
        "        super(DynamicSharedModel, self).__init__(**kwargs)\n",
        "        self.base_model = base_model\n",
        "        self.shared_layer = shared_layer\n",
        "\n",
        "    def call(self, inputs, domain_index=0):\n",
        "        x = self.base_model(inputs)\n",
        "        return self.shared_layer(x, domain_index)\n",
        "\n",
        "new_model = DynamicSharedModel(base_model=distilbert_model, shared_layer=shared_layer)\n",
        "new_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=1, factor=0.5, min_lr=0.00001)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=1)\n",
        "callbacks_list = [lr_reduction, early_stopping, model_checkpoint, tensorboard]\n",
        "\n",
        "# Train the model\n",
        "history = new_model.fit(X_train_distilbert, y_train, batch_size=64, epochs=10, validation_split=0.2, callbacks=callbacks_list, shuffle=True)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = new_model.predict(X_test_distilbert)\n",
        "\n",
        "# Metrics\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "class_names = ['class1', 'class2', 'class3']\n",
        "classification_rep = classification_report(y_test_classes, y_pred_classes, target_names=class_names)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)\n",
        "macro_f1 = f1_score(y_test_classes, y_pred_classes, average='macro')\n",
        "print('Macro F1 Score:', macro_f1)\n",
        "cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "rmse = sqrt(mean_squared_error(y_test_classes, y_pred_classes))\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    history1 = new_model.fit(X_train_distilbert_domain1, y_train_domain1, batch_size=64, epochs=1, validation_split=0.2, callbacks=callbacks_list, shuffle=True, domain_index=0)\n",
        "    history2 = new_model.fit(X_train_distilbert_domain2, y_train_domain2, batch_size=64, epochs=1, validation_split=0.2, callbacks=callbacks_list, shuffle=True, domain_index=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "BuT9B74jumfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistic"
      ],
      "metadata": {
        "id": "Vb2vR3z74tpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit_posthocs"
      ],
      "metadata": {
        "id": "qyDU65Hqnxj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import rankdata, friedmanchisquare\n",
        "import scikit_posthocs as sp\n",
        "import numpy as np\n",
        "from scipy.stats import rankdata, friedmanchisquare\n",
        "import scikit_posthocs as sp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "tfidf_folds_covid = [0.757, 0.773, 0.775, 0.762, 0.765]\n",
        "word2vec_folds_covid = [0.796, 0.806, 0.806, 0.787, 0.807]\n",
        "fasttext_folds_covid = [0.773, 0.790, 0.793, 0.778, 0.778]\n",
        "glove_folds_covid = [0.731, 0.750, 0.757, 0.733, 0.748]\n",
        "roberta_folds_covid = [0.8385, 0.9066, 0.9430, 0.9539, 0.9578]\n",
        "\n",
        "tfidf_folds_mental = [0.916,0.917,0.917,0.920,0.924]\n",
        "word2vec_folds_mental = [0.909, 0.907, 0.900, 0.911, 0.909]\n",
        "fasttext_folds_mental = [0.912, 0.909, 0.909, 0.909, 0.909]\n",
        "glove_folds_mental = [0.896, 0.898,0.898,0.876,0.902]\n",
        "roberta_folds_mental = [0.9410,0.9778,0.9845,0.9852,0.9859]\n",
        "\n",
        "tfidf_folds_drug = [0.727,0.725,0.726,0.726,0.727]\n",
        "word2vec_folds_drug = [0.693,0.716,0.701,0.710,0.713]\n",
        "fasttext_folds_drug = [0.729,0.715,0.695,0.706,0.706]\n",
        "glove_folds_drug = [0.715,0.711,0.721,0.688,0.678]\n",
        "roberta_folds_drug = [0.715,0.7367,0.8939,0.9110,0.9318]\n",
        "\n",
        "experiments = {\n",
        "    'covid': [tfidf_folds_covid, word2vec_folds_covid, fasttext_folds_covid, glove_folds_covid, roberta_folds_covid],\n",
        "    'mental': [tfidf_folds_mental, word2vec_folds_mental, fasttext_folds_mental, glove_folds_mental, roberta_folds_mental],\n",
        "    'drug': [tfidf_folds_drug, word2vec_folds_drug, fasttext_folds_drug, glove_folds_drug, roberta_folds_drug]\n",
        "}\n",
        "\n",
        "alpha = 0.05\n",
        "methods = ['tfidf', 'word2vec', 'fasttext', 'glove', 'roberta']\n",
        "\n",
        "for experiment, data in experiments.items():\n",
        "    print(f\"Experiment: {experiment.upper()}\")\n",
        "    # Friedman Test\n",
        "    stat, p = friedmanchisquare(*data)\n",
        "    print(f\"Friedman Test Statistic: {stat}\")\n",
        "    print(f\"P-value: {p}\")\n",
        "\n",
        "    if p < alpha:\n",
        "        print(\"Reject the null hypothesis - No significant differences exist in the performances of the methods.\")\n",
        "\n",
        "        # Calculate average ranks\n",
        "        scores = np.array(data)\n",
        "        ranks = np.array([rankdata([-score for score in fold]) for fold in zip(*scores)])  # -score for descending order\n",
        "        avg_ranks = np.mean(ranks, axis=0)\n",
        "        method_ranks = dict(zip(methods, avg_ranks))\n",
        "        print(\"Average Ranks:\", method_ranks)\n",
        "\n",
        "        # Nemenyi post-hoc test\n",
        "        pc = sp.posthoc_nemenyi_friedman(np.array(data).T)\n",
        "        print(\"P-values matrix from Nemenyi post-hoc test:\")\n",
        "        print(pc)\n",
        "        print(\"-\" * 50)\n",
        "    else:\n",
        "        print(\"Fail to reject the null hypothesis - No significant differences exist in the performances of the methods.\")\n",
        "        print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "RqwIxMR9nvN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import rankdata, friedmanchisquare\n",
        "import scikit_posthocs as sp\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "tfidf_folds_covid = [0.757, 0.773, 0.775, 0.762, 0.765]\n",
        "word2vec_folds_covid = [0.796, 0.806, 0.806, 0.787, 0.807]\n",
        "fasttext_folds_covid = [0.773, 0.790, 0.793, 0.778, 0.778]\n",
        "glove_folds_covid = [0.731, 0.750, 0.757, 0.733, 0.748]\n",
        "distillbert_folds_covid = [0.8385, 0.9066, 0.9430, 0.9539, 0.9578]\n",
        "\n",
        "tfidf_folds_mental = [0.916,0.917,0.917,0.920,0.924]\n",
        "word2vec_folds_mental = [0.909, 0.907, 0.900, 0.911, 0.909]\n",
        "fasttext_folds_mental = [0.912, 0.909, 0.909, 0.909, 0.909]\n",
        "glove_folds_mental = [0.896, 0.898,0.898,0.876,0.902]\n",
        "distillbert_folds_mental = [0.9410,0.9778,0.9845,0.9852,0.9859]\n",
        "\n",
        "tfidf_folds_drug = [0.727,0.725,0.726,0.726,0.727]\n",
        "word2vec_folds_drug = [0.693,0.716,0.701,0.710,0.713]\n",
        "fasttext_folds_drug = [0.729,0.715,0.695,0.706,0.706]\n",
        "glove_folds_drug = [0.715,0.711,0.721,0.688,0.678]\n",
        "distillbert_folds_drug = [0.715,0.7367,0.8939,0.9110,0.9318]\n",
        "\n",
        "experiments = {\n",
        "    'covid dataset': [tfidf_folds_covid, word2vec_folds_covid, fasttext_folds_covid, glove_folds_covid, distillbert_folds_covid],\n",
        "    'mental dataset': [tfidf_folds_mental, word2vec_folds_mental, fasttext_folds_mental, glove_folds_mental, distillbert_folds_mental],\n",
        "    'drug dataset': [tfidf_folds_drug, word2vec_folds_drug, fasttext_folds_drug, glove_folds_drug, distillbert_folds_drug]\n",
        "}\n",
        "\n",
        "# Initialize parameters\n",
        "alpha = 0.05\n",
        "methods = ['tfidf', 'word2vec', 'fasttext', 'glove', 'distillbert']\n",
        "\n",
        "# List to store the p-value matrices from each experiment\n",
        "pc_list = []\n",
        "\n",
        "for experiment, data in experiments.items():\n",
        "    print(f\"\\n\\n--- Experiment: {experiment.upper()} ---\")\n",
        "\n",
        "    # Friedman Test\n",
        "    stat, p = friedmanchisquare(*data)\n",
        "    print(f\"Friedman Test Statistic: {stat:.4f}\")\n",
        "    print(f\"P-value: {p:.4f}\")\n",
        "\n",
        "    if p < alpha:\n",
        "        print(\"\\n[RESULT] Reject the null hypothesis: Significant differences exist in the performances of the methods.\\n\")\n",
        "\n",
        "        # Calculate average ranks\n",
        "        scores = np.array(data)\n",
        "        ranks = np.array([rankdata([-score for score in fold]) for fold in zip(*scores)])  # -score for descending order\n",
        "        avg_ranks = np.mean(ranks, axis=0)\n",
        "        method_ranks = dict(zip(methods, avg_ranks))\n",
        "        print(\"Average Ranks:\", method_ranks)\n",
        "\n",
        "        # Nemenyi post-hoc test\n",
        "        pc = sp.posthoc_nemenyi_friedman(np.array(data).T)\n",
        "        pc_list.append(pc)  # Store the p-value matrix\n",
        "        print(\"\\nP-values matrix from Nemenyi post-hoc test:\\n\")\n",
        "        print(pc)\n",
        "\n",
        "    else:\n",
        "        print(\"\\n[RESULT] Fail to reject the null hypothesis: No significant differences exist in the performances of the methods.\")\n",
        "\n",
        "# Visualization outside of the loop, plotting all three side by side\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "for ax, pc, experiment in zip(axes, pc_list, experiments):\n",
        "    sns.heatmap(pc, annot=True, cmap='coolwarm', cbar=True, square=True,\n",
        "                fmt='.3f', xticklabels=methods, yticklabels=methods, ax=ax)\n",
        "    ax.set_title(f\"P-values for {experiment.upper()}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "SoqqL2q7L5R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.heatmap(pc, annot=True, cmap='coolwarm', cbar=True, square=True, fmt='.3f', xticklabels=methods, yticklabels=methods)\n",
        "plt.title(f\"P-values from Nemenyi post-hoc test for {experiment.upper()}\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Sm8rzjGpLYsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the style for academic publications\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "# Create a figure and axis objects\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 6))  # 3 columns for 3 experiments\n",
        "\n",
        "# Use a grayscale color palette\n",
        "colors = ['lightgray', '0.7', '0.5', '0.3', 'black']  # Example grayscale colors\n",
        "\n",
        "# Iterate through experiments and plot the box plots side by side\n",
        "for ax, (experiment, data) in zip(axes, experiments.items()):\n",
        "    ax.boxplot(data, vert=True, patch_artist=True, labels=methods)\n",
        "    ax.set_title(f\"{experiment.upper()}\")\n",
        "    ax.set_ylabel('Score')  # You can remove individual y labels if you want a common one\n",
        "    ax.grid(axis='y', linestyle='--')\n",
        "\n",
        "    # Fill boxes with grayscale colors\n",
        "    for patch, color in zip(ax.artists, colors):\n",
        "        patch.set_facecolor(color)\n",
        "\n",
        "# Add common title and y-label if necessary\n",
        "fig.suptitle('Box plot Results Across Experiments based on Micro F1 Scores', fontsize=16)\n",
        "fig.text(0.08, 0.5, 'Score', va='center', rotation='vertical', fontsize=12)  # Common y-label\n",
        "\n",
        "# Display the plots\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.85)  # Adjust top spacing for the suptitle\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "uJwaLb5f6BD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for experiment, data in experiments.items():\n",
        "    print(f\"Box plot statistics for {experiment.upper()}:\")\n",
        "\n",
        "    for method, values in zip(methods, data):\n",
        "        min_val = np.min(values)\n",
        "        q1 = np.percentile(values, 25)\n",
        "        median = np.median(values)\n",
        "        q3 = np.percentile(values, 75)\n",
        "        max_val = np.max(values)\n",
        "\n",
        "        print(f\"{method}:\")\n",
        "        print(f\"  Min: {min_val:.3f}\")\n",
        "        print(f\"  Q1: {q1:.3f}\")\n",
        "        print(f\"  Median: {median:.3f}\")\n",
        "        print(f\"  Q3: {q3:.3f}\")\n",
        "        print(f\"  Max: {max_val:.3f}\")\n",
        "\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "x7PptF5g7MiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import wilcoxon\n",
        "\n",
        "word2vec_folds = [0.796, 0.806, 0.806, 0.787, 0.807]\n",
        "roberta_folds = [0.8385, 0.9066, 0.9430, 0.9539, 0.9578]\n",
        "\n",
        "stat, p = wilcoxon(word2vec_folds, roberta_folds)\n",
        "print(f\"Wilcoxon Signed-Rank Test between word2vec and roberta:\")\n",
        "print(f\"Statistic: {stat}\")\n",
        "print(f\"P-value: {p}\")\n"
      ],
      "metadata": {
        "id": "chlaM_SRu9r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# Labels for the methods\n",
        "labels = [\"word2vec\", \"fasttext\", \"glove\", \"roberta\"]\n",
        "\n",
        "# Create the heatmap using seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(pc, annot=True, cmap=\"coolwarm\", xticklabels=labels, yticklabels=labels, linewidths=.5, cbar_kws={'label': 'P-value'})\n",
        "\n",
        "# Decorations\n",
        "plt.title(\"Nemenyi Post-hoc Test Results\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GKcJ9coXoPVC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}